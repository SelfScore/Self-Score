# Level 4 Realtime Voice Interview - Complete Implementation Plan

## ğŸ“‹ Executive Summary

This document outlines the complete implementation of a real-time, voice-based interview system for Level 4 assessment using:

- **WebRTC** for bidirectional audio streaming
- **Gemini 2.5 Realtime Voice API** for natural voice interaction
- **Streaming STT** for transcript capture
- **Gemini Flash** for answer analysis
- **Backend-orchestrated flow** (AI never controls progression)

---

## ğŸ¯ Core Principles

1. âœ… **Backend controls everything** - AI is just a voice interface
2. âœ… **No interruptions** - Let users speak freely
3. âœ… **Per-question transcripts** - Clean answer separation
4. âœ… **Confidence-based progression** - Smart follow-ups
5. âœ… **Stateless per user** - One session object per interview

---

## ğŸ—ï¸ System Architecture

### High-Level Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         WebRTC Audio        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Backend API    â”‚
â”‚  (React)    â”‚         (bidirectional)     â”‚   (Node.js)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                â”‚                â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                          â”‚ Gemini Voice  â”‚  â”‚  STT Stream â”‚  â”‚   Gemini   â”‚
                          â”‚  (Realtime)   â”‚  â”‚  (Capture)  â”‚  â”‚   Flash    â”‚
                          â”‚               â”‚  â”‚             â”‚  â”‚ (Analysis) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component           | Purpose                          | Technology                |
| ------------------- | -------------------------------- | ------------------------- |
| **Frontend**        | Capture mic, play audio, show UI | React + WebRTC            |
| **Backend API**     | Orchestrate flow, store data     | Express + WebSocket       |
| **Session Manager** | Track active interviews          | In-memory Map             |
| **State Machine**   | Per-question progress tracking   | Custom logic              |
| **Gemini Realtime** | Voice-to-voice interaction       | Gemini 2.5 Live API       |
| **STT Stream**      | Real-time transcription          | Google Cloud STT / Gemini |
| **Gemini Flash**    | Answer analysis (async)          | Gemini 2.5 Flash          |
| **MongoDB**         | Persist interview results        | Mongoose models           |

---

## ğŸ“Š Data Models

### 1. RealtimeInterview Model (New - Replaces AIInterview)

```typescript
{
  _id: ObjectId,
  userId: ObjectId,
  sessionId: string (UUID),
  level: 4,
  status: "IN_PROGRESS" | "COMPLETED" | "ABANDONED",

  // NOTE: Expanded to 25 questions (from 8)
  questions: [{
    questionId: string,
    questionText: string,
    order: number
  }],

  answers: [{
    questionId: string,
    transcript: string,              // Verbatim answer
    confidence: number,              // 0-100 (from Gemini Flash)
    isComplete: boolean,
    isOffTopic: boolean,
    missingAspects: string[],
    followUpAsked: boolean,
    audioTimestamp: {
      start: Date,
      end: Date
    }
  }],

  interviewMetadata: {
    totalDuration: number,           // seconds
    averageAnswerLength: number,     // seconds
    followUpCount: number,
    redirectionCount: number
  },

  startedAt: Date,
  completedAt: Date,
  submittedAt: Date
}
```

### 2. Interview Session (In-Memory)

```typescript
interface InterviewSession {
  sessionId: string;
  userId: string;
  interviewId: string;

  // State machine
  currentQuestionIndex: number;
  questions: Question[];
  answers: Map<string, AnswerState>;

  // Connections
  geminiConnection: GeminiRealtimeConnection;
  sttStream: SpeechToTextStream;
  webrtcPeer: RTCPeerConnection;

  // Timing & control
  lastAudioTimestamp: number;
  silenceTimer: NodeJS.Timeout | null;
  isAISpeaking: boolean;
  isUserSpeaking: boolean;

  // Analysis queue
  pendingAnalysis: Promise<any> | null;
}

interface AnswerState {
  questionId: string;
  transcript: string;
  confidence: number;
  isComplete: boolean;
  isOffTopic: boolean;
  missingAspects: string[];
  followUpAsked: boolean;
}
```

---

## ğŸ”„ Complete Interview Flow

### Phase 1: Initialization

```
1. User clicks "Start Voice Interview"
   â”‚
   â”œâ”€â–º Frontend: POST /api/realtime-interview/start
   â”‚
   â”œâ”€â–º Backend:
   â”‚    â”œâ”€ Create InterviewSession
   â”‚    â”œâ”€ Generate sessionId (UUID)
   â”‚    â”œâ”€ Fetch 8 questions from MongoDB
   â”‚    â”œâ”€ Initialize state machine
   â”‚    â””â”€ Return: { sessionId, webrtcOffer }
   â”‚
   â””â”€â–º Frontend: Establish WebRTC connection
```

### Phase 2: WebRTC Setup

```
2. WebRTC Connection
   â”‚
   â”œâ”€â–º Frontend sends WebRTC offer
   â”‚
   â”œâ”€â–º Backend creates peer connection
   â”‚    â”œâ”€ Setup audio track handlers
   â”‚    â”œâ”€ Route user audio â†’ Session
   â”‚    â””â”€ Route AI audio â†’ Frontend
   â”‚
   â””â”€â–º Connection established (audio flows both ways)
```

### Phase 3: Interview Start

```
3. Backend sends first instruction to Gemini:
   â”‚
   â”œâ”€â–º System Message (once): "You are a mental health interviewer..."
   â”‚
   â”œâ”€â–º Control Instruction:
   â”‚    "Ask this question verbatim: [Question 1 text]
   â”‚     Then stay completely silent and listen."
   â”‚
   â””â”€â–º Gemini speaks Question 1
```

### Phase 4: User Answers (Core Loop)

```
4. While user is speaking:
   â”‚
   â”œâ”€â–º Audio Frame received from frontend
   â”‚    â”‚
   â”‚    â”œâ”€â–º Send to Gemini (for conversation continuity)
   â”‚    â”‚
   â”‚    â””â”€â–º Send to STT (for transcript capture)
   â”‚
   â”œâ”€â–º STT produces partial transcripts
   â”‚    â””â”€â–º Append to current answer buffer
   â”‚
   â”œâ”€â–º User pauses (silence detected)
   â”‚    â”‚
   â”‚    â”œâ”€â–º STT produces final transcript
   â”‚    â”‚
   â”‚    â”œâ”€â–º Trigger Gemini Flash analysis (async)
   â”‚    â”‚    Input: {
   â”‚    â”‚      question: "...",
   â”‚    â”‚      answer: "...",
   â”‚    â”‚      previousContext: "..."
   â”‚    â”‚    }
   â”‚    â”‚    Output: {
   â”‚    â”‚      confidence: 0-100,
   â”‚    â”‚      isComplete: boolean,
   â”‚    â”‚      isOffTopic: boolean,
   â”‚    â”‚      missingAspects: string[]
   â”‚    â”‚    }
   â”‚    â”‚
   â”‚    â””â”€â–º Wait for analysis result (max 2 seconds)
   â”‚
   â””â”€â–º Decision Engine evaluates next action
```

### Phase 5: Decision Engine

```
5. After silence + analysis:
   â”‚
   â”œâ”€â–º If confidence >= 80 && isComplete:
   â”‚    â””â”€â–º Mark question complete
   â”‚         â””â”€â–º Move to next question
   â”‚              â””â”€â–º Send instruction: "Ask Question N+1"
   â”‚
   â”œâ”€â–º If isOffTopic:
   â”‚    â””â”€â–º Send instruction: "Gently redirect to: [topic]"
   â”‚
   â”œâ”€â–º If confidence < 80 && !followUpAsked:
   â”‚    â””â”€â–º Send instruction: "Ask: Can you elaborate on [aspect]?"
   â”‚
   â””â”€â–º If still speaking (no real silence):
         â””â”€â–º Continue listening
```

### Phase 6: Interview Completion

```
6. After all 8 questions:
   â”‚
   â”œâ”€â–º Send final instruction: "Thank the user and end."
   â”‚
   â”œâ”€â–º Gemini speaks closing message
   â”‚
   â”œâ”€â–º Backend saves to MongoDB:
   â”‚    â”œâ”€ Interview document with all answers
   â”‚    â”œâ”€ Update user progress (Level 4 â†’ PENDING_REVIEW)
   â”‚    â””â”€ Send notification emails
   â”‚
   â”œâ”€â–º Cleanup:
   â”‚    â”œâ”€ Close Gemini connection
   â”‚    â”œâ”€ Close STT stream
   â”‚    â”œâ”€ Close WebRTC peer
   â”‚    â””â”€ Delete session from memory
   â”‚
   â””â”€â–º Return: { success: true, interviewId }
```

---

## ğŸ› ï¸ Implementation Phases

### âœ… PHASE 1: Backend Infrastructure (Week 1)

#### 1.1 Project Setup

- [ ] Install dependencies:
  ```bash
  npm install ws simple-peer @google-cloud/speech uuid
  npm install @google/generative-ai@latest
  ```
- [ ] Configure environment variables
- [ ] Setup TypeScript types

#### 1.2 Session Manager

- [ ] Create `SessionRegistry` class
  - `createSession(userId, interviewId): sessionId`
  - `getSession(sessionId): InterviewSession`
  - `deleteSession(sessionId): void`
- [ ] Implement session timeout cleanup

#### 1.3 State Machine

- [ ] Create `InterviewStateMachine` class
  - `currentQuestion(): Question`
  - `recordAnswer(transcript): void`
  - `completeQuestion(): void`
  - `getProgress(): Progress`
  - `getFinalAnswers(): Answer[]`

#### 1.4 MongoDB Models

- [ ] Create `RealtimeInterview` model
- [ ] Add migration script for existing data
- [ ] Create indexes for performance

---

### âœ… PHASE 2: WebRTC & Audio Streaming (Week 1-2)

#### 2.1 WebRTC Server Setup

- [ ] Create WebRTC signaling endpoint
- [ ] Handle SDP offer/answer exchange
- [ ] Setup ICE candidate handling
- [ ] Route audio tracks to sessions

#### 2.2 Audio Pipeline

- [ ] Create audio buffer management
- [ ] Implement audio frame routing:
  - User audio â†’ Gemini
  - User audio â†’ STT
  - AI audio â†’ Frontend
- [ ] Handle audio format conversion if needed

---

### âœ… PHASE 3: Gemini Realtime Voice Integration (Week 2)

#### 3.1 Gemini Connection Manager

- [ ] Create `GeminiRealtimeConnection` class
- [ ] Implement WebSocket connection to Gemini Live API
- [ ] Handle connection lifecycle (open, error, close)
- [ ] Setup audio streaming to Gemini

#### 3.2 System Instructions

- [ ] Define stable system prompt (sent once):
  ```
  You are a compassionate mental health interviewer.
  Rules:
  - Never diagnose or provide medical advice
  - Speak only when explicitly instructed
  - Never interrupt the user
  - Use warm, empathetic tone
  - Keep questions concise
  ```

#### 3.3 Control Instruction System

- [ ] Implement instruction templates:
  - `askQuestion(questionText)`
  - `followUp(aspect)`
  - `redirect(topic)`
  - `closeInterview()`
- [ ] Send instructions as hidden messages to Gemini
- [ ] Handle Gemini audio responses

---

### âœ… PHASE 4: Speech-to-Text Integration (Week 2)

#### 4.1 STT Service Setup

- [ ] Initialize Google Cloud STT client
- [ ] Create streaming recognition configuration
- [ ] Handle real-time transcription

#### 4.2 Transcript Management

- [ ] Buffer partial transcripts
- [ ] Detect final transcripts
- [ ] Append to current answer state
- [ ] Store complete transcripts per question

#### 4.3 Deepgram Configuration

- [ ] Setup Deepgram API client
- [ ] Configure streaming recognition
- [ ] Handle interim and final results
- [ ] Error handling and reconnection

---

### âœ… PHASE 5: Answer Analysis with Gemini Flash (Week 3)

#### 5.1 Analysis Service

- [ ] Create `AnswerAnalyzer` class
- [ ] Implement async analysis function:
  ```typescript
  analyzeAnswer(
    question: string,
    answer: string,
    context?: string
  ): Promise<AnalysisResult>
  ```

#### 5.2 Analysis Prompt Engineering

- [ ] Design analysis prompt:

  ```
  Question: {question}
  User Answer: {answer}

  Evaluate:
  1. Completeness (0-100)
  2. Is on-topic (yes/no)
  3. Missing aspects (list)

  Return JSON.
  ```

- [ ] Parse and validate results
- [ ] Handle analysis errors gracefully

---

### âœ… PHASE 6: Turn-Taking & Interruption Logic (Week 3)

#### 6.1 Silence Detection

- [ ] Track last audio timestamp
- [ ] Implement silence threshold (2-3 seconds)
- [ ] Fire silence handler callback

#### 6.2 Decision Engine

- [ ] Create `InterviewController` class
- [ ] Implement decision logic:
  ```typescript
  onSilence(session: InterviewSession): Action {
    if (offTopic) return "redirect";
    if (incomplete && !followedUp) return "followUp";
    if (confident) return "nextQuestion";
    return "continue";
  }
  ```
- [ ] Convert decisions to Gemini instructions
- [ ] Handle edge cases (very long answers, no speech)

---

### âœ… PHASE 7: API Endpoints (Week 3-4)

#### 7.1 REST Endpoints

- [ ] `POST /api/realtime-interview/start`
  - Create session
  - Return sessionId + WebRTC offer
- [ ] `POST /api/realtime-interview/connect`
  - WebRTC answer exchange
  - Establish peer connection
- [ ] `POST /api/realtime-interview/end`
  - Graceful shutdown
  - Save results

#### 7.2 WebSocket Endpoints

- [ ] Setup WebSocket server for control messages
- [ ] Handle client events:
  - `audio-data`: Incoming audio frames
  - `silence-detected`: User paused
  - `resume-speaking`: User continued
  - `end-interview`: User ended early

---

### âœ… PHASE 8: Frontend Component (Week 4)

#### 8.1 Level4VoiceTest.tsx (Complete Rewrite)

- [ ] Remove old AI interview logic
- [ ] Implement WebRTC client:
  - Get user media (microphone)
  - Create peer connection
  - Send/receive audio streams
- [ ] UI Components:
  - [ ] Question display
  - [ ] Waveform visualization (optional)
  - [ ] Speaking indicator
  - [ ] Progress tracker
  - [ ] Pause/Resume controls
  - [ ] End interview button

#### 8.2 Audio Handling

- [ ] Capture microphone audio
- [ ] Send audio chunks to backend
- [ ] Receive and play AI audio
- [ ] Handle audio permissions

---

### âœ… PHASE 9: Persistence & Cleanup (Week 4)

#### 9.1 Data Persistence

- [ ] Save interview on completion:
  - Interview metadata
  - Per-question answers
  - Confidence scores
  - Timestamps
- [ ] Update user progress:
  - Set `level4Status` to `PENDING_REVIEW`
  - Record attempt number

#### 9.2 Cleanup Logic

- [ ] Implement graceful shutdown:
  ```typescript
  async cleanup(sessionId: string) {
    const session = getSession(sessionId);
    await session.geminiConnection.close();
    await session.sttStream.close();
    session.webrtcPeer.close();
    deleteSession(sessionId);
  }
  ```
- [ ] Handle disconnections
- [ ] Auto-cleanup on timeout (30 min idle)

---

### âœ… PHASE 10: Testing & Optimization (Week 5)

#### 10.1 Unit Tests

- [ ] Test state machine transitions
- [ ] Test decision engine logic
- [ ] Test analysis parsing

#### 10.2 Integration Tests

- [ ] Test full interview flow (mock audio)
- [ ] Test error scenarios
- [ ] Test cleanup on crash

#### 10.3 Load Testing

- [ ] Test concurrent interviews
- [ ] Monitor memory usage
- [ ] Optimize session cleanup

#### 10.4 End-to-End Testing

- [ ] Test with real users
- [ ] Validate audio quality
- [ ] Check transcript accuracy

---

## ğŸ“ File Structure

```
server/src/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ realtimeInterview.ts           # New MongoDB model
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ sessionManager.ts              # Session registry
â”‚   â”œâ”€â”€ geminiRealtimeService.ts       # Gemini Live API
â”‚   â”œâ”€â”€ sttService.ts                  # Speech-to-Text
â”‚   â”œâ”€â”€ answerAnalyzer.ts              # Gemini Flash analysis
â”‚   â””â”€â”€ webrtcService.ts               # WebRTC handling
â”‚
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ realtimeInterview.controller.ts # Main endpoints
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ interviewStateMachine.ts       # Core state logic
â”‚   â”œâ”€â”€ interviewController.ts         # Decision engine
â”‚   â””â”€â”€ audioRouter.ts                 # Audio pipeline
â”‚
â””â”€â”€ routes/
    â””â”€â”€ realtimeInterview.ts           # API routes

client/src/app/user/test/
â”œâ”€â”€ Level4VoiceTest.tsx                # Complete rewrite
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ AudioVisualizer.tsx            # Waveform display
â”‚   â””â”€â”€ InterviewControls.tsx          # Pause/End buttons
â””â”€â”€ hooks/
    â””â”€â”€ useWebRTC.ts                   # WebRTC logic

```

---

## ğŸ” Security Considerations

1. **API Key Protection**

   - Never expose Gemini API key to frontend
   - Use environment variables
   - Rotate keys regularly

2. **Session Validation**

   - Validate sessionId on every request
   - Check userId ownership
   - Implement rate limiting

3. **Audio Security**

   - Use HTTPS for WebRTC signaling
   - Validate audio format/size
   - Prevent audio injection attacks

4. **Data Privacy**
   - Encrypt interview data at rest
   - Implement data retention policy
   - Allow user data deletion (GDPR)

---

## ğŸš€ Deployment Considerations

### Environment Variables

``Deepgram API (for STT)
DEEPGRAM_API_KEY=your_deepgram_key_here
GEMINI_REALTIME_MODEL=gemini-2.5-flash
GEMINI_VOICE_MODEL=gemini-2.5-realtime

# Google Cloud (for STT)

GOOGLE_CLOUD_PROJECT_ID=your_project
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# Server Config

WEBRTC_PORT=8080
SESSION_TIMEOUT_MS=1800000
SILENCE_THRESHOLD_MS=3000
MAX_CONCURRENT_SESSIONS=100

```

### Infrastructure

- **WebRTC Server**: Needs public IP or TURN server
- **WebSocket Support**: For Gemini + control messages
- **Memory**: ~50MB per active session (monitor closely)
- **Bandwidth**: ~64 kbps per audio stream Ã— 2 (up + down)

---

## ğŸ“Š Monitoring & Metrics

### Key Metrics to Track

1. **Session Metrics**
   - Active sessions count
   - Average session duration
   - Session completion rate

2. **Audio Quality**
   - Audio frame loss rate
   - Latency (end-to-end)
   - STT accuracy (manual sampling)

3. **Analysis Quality**
   - Average confidence scores
   - Follow-up frequency
   - Redirection frequency

4. **System Health**
   - Memory usage per session
   - WebRTC connection failures
   - Gemini API errors
   - STT timeouts

---

## âš ï¸ Known Challenges & Solutions

### Challenge 1: Audio Sync Issues
**Problem**: AI voice and user voice might overlap
**Solution**:
- Track `isAISpeaking` flag
- Buffer user audio during AI speech
- Use voice activity detection (VAD)

### Challenge 2: Long Pauses
**Problem**: User might pause mid-thought
**Solution**:
- Use adaptive silence threshold
- Check if answer seems incomplete
- Don't rush to next question

### Challenge 3: Off-Topic Rambling
**Problem**: User goes way off-topic
**Solution**:
- Run analysis every 30 seconds during long answers
- Gently redirect if off-topic detected
- Allow 1-2 redirects per question max

### Challenge 4: Connection Drops
**Problem**: WebRTC disconnects mid-interview
**Solution**:
- Save transcript every 10 seconds
- Implement reconnection logic
- Allow resume from last question

---

## ğŸ¯ Success Criteria

### Functional Requirements
- âœ… Complete 8-question interview without crashes
- âœ… Clean per-question answer separation
- âœ… No AI interruptions during user speech
- âœ… Follow-ups only when needed (< 2 per question)
- âœ… Accurate transcription (> 90% word accuracy)

### Performance Requirements
- âœ… Audio latency < 500ms
- âœ… Analysis results < 2 seconds
- âœ… Support 10+ concurrent interviews
- âœ… Memory usage < 100MB per session

### User Experience
- âœ… Natural conversation flow
- âœ… Clear audio quality
- âœ… Smooth transitions between questions
- âœ… Obvious when AI is speaking vs listening

---

## ğŸ“ Next Steps

1. **Confirm Decisions** (This document)
   - Agree on question count (8 vs 25)
   - Confirm Gemini API access
   - Choose STT provider

2. **Start Implementation**
   - Begin with Phase 1 (Backend Infrastructure)
   - Setup development environment
   - Create base models and types

3. **Iterative Development**
   - Build and test each phase
   - Get user feedback early
   - Adjust based on real usage

---

## âœ… DECISIONS CONFIRMED (18 Dec 2025)

1. **Question Count**: **25 questions** (expand from current 8)
2. **API Access**: âœ… **Gemini 2.5 Realtime/Live API enabled**
3. **STT Provider**: **Deepgram API** (not Google Cloud STT)
4. **Integration Strategy**: **Complete replacement** of existing system
5. **Database Models**: **Migrate to new RealtimeInterview model**
6. **Level4VoiceTest.tsx**: **Complete rewrite** with WebRTC
7. **Implementation**: **Phase-by-phase, step-by-step**

---

## ğŸ“ˆ IMPLEMENTATION PROGRESS TRACKER

### âœ… COMPLETED
- [x] Implementation plan document created
- [x] Architecture decisions finalized
- [x] STT provider selected (Deepgram)
- [x] **Phase 1: Backend Infrastructure** âœ… COMPLETE
  - [x] Dependencies installed (ws, @deepgram/sdk, uuid, simple-peer)
  - [x] Session Manager created (`services/sessionManager.ts`)
  - [x] State Machine implemented (`lib/interviewStateMachine.ts`)
  - [x] MongoDB models created
    - [x] RealtimeInterview model (`models/realtimeInterview.ts`)
    - [x] TypeScript types (`types/realtimeInterview.types.ts`)
    - [x] 25 questions seed script (`scripts/seedLevel4Questions.ts`)
- [x] **Phase 2 & 4: WebRTC + Deepgram Integration** âœ… COMPLETE
  - [x] Deepgram streaming service (`services/deepgramService.ts`)
  - [x] WebSocket handler (`lib/websocketHandler.ts`)
  - [x] Real-time audio pipeline
- [x] **Phase 3 & 5: Gemini Integration** âœ… COMPLETE (18 Dec 2025)
  - [x] Gemini Realtime Voice API FULLY IMPLEMENTED (`services/geminiRealtimeService.ts`)
  - [x] Real WebSocket connection to Gemini Live API
  - [x] Audio streaming TO Gemini (user voice)
  - [x] Audio streaming FROM Gemini (AI voice)
  - [x] System prompt and configuration
  - [x] Instruction system (ask_question, follow_up, redirect, close)
  - [x] Answer Analyzer with Gemini Flash (`services/answerAnalyzer.ts`)
  - [x] Follow-up and redirect generation
- [x] **Phase 6 & 7: Controllers & API** âœ… COMPLETE
  - [x] Interview Controller / Decision Engine (`lib/interviewController.ts`)
  - [x] REST API endpoints (`controllers/realtimeInterview.controller.ts`)
  - [x] Routes configured (`routes/realtimeInterview.ts`)
  - [x] WebSocket integration in main server
- [x] **Phase 8: Frontend Component** âœ… COMPLETE
  - [x] Level4VoiceTest.tsx complete rewrite
  - [x] WebSocket client implementation
  - [x] Audio capture and playback (MediaRecorder)
  - [x] UI components (progress bar, transcript display, controls)
  - [x] Real-time transcript display with interim/final states
  - [x] Recording controls with pause/resume
  - [x] Interview completion flow with confirmation dialog
  - [x] Error handling and connection status
- [x] **Environment Configuration** âœ… COMPLETE
  - [x] DEEPGRAM_API_KEY configured in server/.env
  - [x] NEXT_PUBLIC_WS_URL added to client/.env.local
  - [x] Backend server running successfully on port 5001
- [x] **Bug Fixes & Optimization** âœ…
  - [x] Fixed TypeScript compilation errors in controllers
  - [x] Fixed user ID field (user._id instead of user.id)
  - [x] Wrapped handlers in useCallback for performance
  - [x] Fixed React dependency arrays

### ğŸš§ IN PROGRESS
- [x] **Phase 9: Persistence & Cleanup** âœ… MOSTLY COMPLETE (18 Dec 2025)
  - [x] Models updated to support 25 questions (Level4Question, Level4Review)
  - [x] WebSocket message format aligned with frontend expectations
  - [x] Question progression notifications implemented
  - [x] Transcript handling fixed (interim/final message types)
  - [x] handleTranscript() method verified in InterviewController
  - [x] TypeScript compilation errors resolved
  - [ ] End-to-end testing with real audio (READY TO TEST)
  - [ ] MongoDB persistence validation
  - [ ] Session cleanup verification

### â³ UPCOMING
- [ ] Phase 9: Complete Persistence & Cleanup testing
- [ ] Phase 10: Testing & Optimization
  - [ ] Load testing with concurrent sessions
  - [ ] Audio quality validation
  - [ ] Transcript accuracy testing
  - [ ] Performance optimization

### ğŸ“ NOTES
- Started: 18 December 2025
- Current Focus: Testing & Validation
- **Backend Complete**: âœ… All 13 backend files created and server running
- **Frontend Complete**: âœ… Level4VoiceTest.tsx fully rewritten with WebSocket client
- **Environment**: âœ… All environment variables configured
- **Database**: âœ… Level 4 questions exist in MongoDB
- **Bug Fixes**: âœ… TypeScript errors resolved, React hooks optimized
- Next Milestone: End-to-end testing with real audio input

---

## ğŸ”„ CHANGE LOG

### 18 Dec 2025 - Phase 3 COMPLETE: Gemini Live Voice API Integration âœ… ğŸ¤
- âœ… **FULL VOICE-TO-VOICE**: Implemented complete Gemini Live API integration
  - Real WebSocket connection to Gemini 2.5 Live API
  - Bidirectional audio streaming (user â†” AI)
  - AI voice responses in real-time (Puck voice, 24kHz)
  - System prompts and interview context
  - Instruction control system (ask, follow-up, redirect, close)
- âœ… **Backend Audio Routing**: Dual routing implemented
  - User audio â†’ Deepgram (transcript) + Gemini (voice interaction)
  - AI audio â†’ Frontend (playback)
  - Smart routing (only when AI not speaking)
- âœ… **Frontend Audio Playback**: Complete implementation
  - Audio context for AI voice (24kHz)
  - Binary audio detection and decoding
  - Automatic playback with state management
  - Audio queue and cleanup
- âœ… **Files Modified**: 4 files, ~450 lines changed
  - `server/src/services/geminiRealtimeService.ts` - Complete rewrite
  - `server/src/lib/websocketHandler.ts` - Gemini init + audio routing
  - `server/src/services/sessionManager.ts` - Connection cleanup
  - `client/src/app/user/test/Level4VoiceTest.tsx` - Audio playback
- âœ… **TypeScript Compilation**: All passing âœ…
- ğŸ¯ **Status**: READY FOR VOICE TESTING - AI WILL SPEAK! ğŸ¤

### 18 Dec 2025 - Phase 9 Progress: Model Updates & Bug Fixes âœ…
- âœ… **Model Updates**: Updated Level4Question and Level4Review models to support 25 questions
  - Level4Question: Changed max order from 8 to 25
  - Level4Review: Changed validation to accept 1-25 question reviews instead of exactly 8
- âœ… **WebSocket Message Format**: Fixed transcript message types
  - Changed from generic `transcript` to `transcript_interim` and `transcript_final`
  - Aligned with frontend expectations in Level4VoiceTest.tsx
- âœ… **Question Progression**: Enhanced InterviewController
  - Added sendWebSocketMessage() helper method
  - Implemented next_question notifications with proper format
  - Added progress updates on question transitions
  - Added interview_complete notification
- âœ… **Initial Question Setup**: Improved WebSocket handler initialization
  - Sends proper next_question message for first question
  - Includes progress update on connection
  - Simulates AI speaking state (placeholder for Gemini voice)
- âœ… **Bug Verification**: Confirmed handleTranscript() method exists and works correctly
- ğŸ¯ **Next Steps**: Ready for end-to-end testing with real audio input

### 18 Dec 2025 - Phase 8 Complete + Bug Fixes âœ…
- âœ… **Frontend Component Rewrite**: Complete Level4VoiceTest.tsx implementation
  - WebSocket client with connection management
  - MediaRecorder for audio capture (16kHz, opus codec)
  - Real-time transcript display (interim/final/question states)
  - Progress tracking with visual indicators
  - Recording controls (pause/resume functionality)
  - End interview confirmation dialog
  - Error handling and connection status alerts
- âœ… **Bug Fixes**:
  - Fixed user ID field: `user._id` instead of `user.id`
  - Wrapped event handlers in `useCallback` for performance
  - Fixed React useEffect dependency arrays
  - Resolved TypeScript compilation errors in controllers
- âœ… **Environment Setup**:
  - NEXT_PUBLIC_WS_URL added to client/.env.local
  - Verified DEEPGRAM_API_KEY in server/.env
  - Backend server running successfully
- ğŸ“Š **System Status**: Fully operational, ready for testing

### 18 Dec 2025 - Phase 2-7 Complete âœ… (Backend Done!)
- âœ… **Deepgram Integration**: Real-time STT with live transcription
- âœ… **WebSocket Server**: Bidirectional audio streaming
- âœ… **Gemini Services**:
  - Answer analyzer using Gemini Flash
  - Follow-up generation
  - Redirect generation
  - Realtime voice service (placeholder for when API is stable)
- âœ… **Interview Controller**: Decision engine with turn-taking logic
- âœ… **REST API Endpoints**:
  - POST `/api/realtime-interview/start`
  - POST `/api/realtime-interview/connect`
  - POST `/api/realtime-interview/complete`
  - GET `/api/realtime-interview/progress/:sessionId`
  - POST `/api/realtime-interview/abandon`
- âœ… **WebSocket Endpoint**: `ws://localhost:5001/ws/interview?sessionId=xxx`
- âœ… **Server Integration**: HTTP + WebSocket server combined
- ğŸ“¦ **Files Created**: 8 new backend files
  - `services/deepgramService.ts`
  - `services/geminiRealtimeService.ts`
  - `services/answerAnalyzer.ts`
  - `lib/interviewController.ts`
  - `lib/websocketHandler.ts`
  - `controllers/realtimeInterview.controller.ts`
  - `routes/realtimeInterview.ts`
  - `index.ts` updated with WebSocket support

### 18 Dec 2025 - Phase 1 Complete âœ…
- âœ… Installed dependencies: ws, @deepgram/sdk, uuid, simple-peer
- âœ… Created `SessionRegistry` singleton for managing active sessions
- âœ… Implemented `InterviewStateMachine` for state progression
- âœ… Created `RealtimeInterview` MongoDB model
- âœ… Defined TypeScript interfaces and types
- âœ… Created seed script for 25 Level 4 questions
- ğŸ“¦ **Files Created**: 5 new files
  - `models/realtimeInterview.ts`
  - `types/realtimeInterview.types.ts`
  - `services/sessionManager.ts`
  - `lib/interviewStateMachine.ts`
  - `scripts/seedLevel4Questions.ts`

### 18 Dec 2025 - Initial Setup
- Created implementation plan document
- Confirmed all architectural decisions
- Ready to begin Phase 1 implementation

---

## ğŸ“š References

- [Gemini API Documentation](https://ai.google.dev/docs)
- [WebRTC MDN Guide](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API)
- [Google Cloud Speech-to-Text](https://cloud.google.com/speech-to-text)
- [Node.js WebSocket](https://github.com/websockets/ws)

---

**Document Version**: 1.0
**Last Updated**: 18 December 2025
**Author**: Development Team
**Status**: Awaiting Approval & Clarifications
```
